model_config:
  hf_id: "TheBloke/Llama-2-7B-Chat-GGUF"
  hf_file: "llama-2-7b-chat.Q4_K_M.gguf"
  n_threads: 2
  n_batch: 512
  n_gpu_layers: -1
  n_ctx: 8196
  system_prompt: true
  system_prompt_start_token: "<s>[INST] <<SYS>>\n"
  system_prompt_end_token: "\n<</SYS>>\n\n"
  user_prompt_start_token: ""
  user_prompt_end_token: " [/INST]"
  user_followup_prompt_start_token: "[INST] "
  user_followup_prompt_end_token: " [/INST]"
  assistant_prompt_start_token: " "
  assistant_prompt_end_token: " </s>"
  assistant_followup_prompt_start_token: " "
  assistant_followup_prompt_end_token: ""
  end_tokens: 
    - "</s>"
  verbose: true
  default_completion_config:
    max_tokens: 8196
    temperature: 0.0
    repeat_penalty: 1.1
    echo: false
    top_p: 1
